{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAM_learn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "16zL6TLy6MZOb6BISV-dsS_kTdoj3DDxk",
      "authorship_tag": "ABX9TyPsX7wBZ+g0G7kffFmFcCL3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changyui/Study/blob/main/SAM_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Preprocess"
      ],
      "metadata": {
        "id": "W580PH7pyJ8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install libpcap-dev"
      ],
      "metadata": {
        "id": "bCVeYZSiwJTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypcap"
      ],
      "metadata": {
        "id": "vFxfkZ7TwYlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dpkt"
      ],
      "metadata": {
        "id": "XiU4IKKsv9jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUYNeMnbvKrj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import dpkt\n",
        "import random\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "protocols = ['dns', 'smtp', 'ssh', 'ftp', 'http', 'https']\n",
        "ports = [53, 25, 22, 21, 80, 443]\n",
        "\n",
        "def gen_flows(pcap):\n",
        "\tflows = [{} for _ in range(len(protocols))]\n",
        "\n",
        "\tif pcap.datalink() != dpkt.pcap.DLT_EN10MB:\n",
        "\t\tprint('unknow data link!')\n",
        "\t\treturn\n",
        "\n",
        "\txgr = 0\n",
        "\tfor _, buff in pcap:\n",
        "\t\teth = dpkt.ethernet.Ethernet(buff)\n",
        "\t\txgr += 1\n",
        "\t\tif xgr % 500000 == 0:\n",
        "\t\t\tprint('The %dth pkt!'%xgr)\n",
        "\t\t\t# break\n",
        "\n",
        "\t\tif isinstance(eth.data, dpkt.ip.IP) and (\n",
        "\t\tisinstance(eth.data.data, dpkt.udp.UDP)\n",
        "\t\tor isinstance(eth.data.data, dpkt.tcp.TCP)):\n",
        "\t\t\t# tcp or udp packet\n",
        "\t\t\tip = eth.data\n",
        "\n",
        "\t\t\t# loop all protocols\n",
        "\t\t\tfor name in protocols:\n",
        "\t\t\t\tindex = protocols.index(name)\n",
        "\t\t\t\tif ip.data.sport == ports[index] or \\\n",
        "\t\t\t\tip.data.dport == ports[index]:\n",
        "\t\t\t\t\tif len(flows[index]) >= 10000:\n",
        "\t\t\t\t\t\t# each class has at most 1w flows\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\t# match a protocol\n",
        "\t\t\t\t\tkey = '.'.join(map(str, map(int, ip.src))) + \\\n",
        "\t\t\t\t\t'>>>' + '.'.join(map(str, map(int, ip.dst))) + \\\n",
        "\t\t\t\t\t':::' + '.'.join(map(str, [ip.p, ip.data.sport, ip.data.dport]))\n",
        "\n",
        "\t\t\t\t\tif key not in flows[index]:\n",
        "\t\t\t\t\t\tflows[index][key] = [ip]\n",
        "\t\t\t\t\telif len(flows[index][key]) < 1000:\n",
        "\t\t\t\t\t\t# each flow has at most 1k flows\n",
        "\t\t\t\t\t\tflows[index][key].append(ip)\n",
        "\t\t\t\t\t# after match a protocol quit\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\treturn flows\n",
        "\n",
        "\n",
        "# def split_train_test(flows, name, k):\n",
        "# \tkeys = list(flows.keys())\n",
        "\n",
        "# \ttest_keys = keys[k*int(len(keys)*0.1):(k+1)*int(len(keys)*0.1)]\n",
        "# \ttest_min = 0xFFFFFFFF\n",
        "# \ttest_flows = {}\n",
        "# \tfor k in test_keys:\n",
        "# \t\ttest_flows[k] = flows[k]\n",
        "# \t\ttest_min = min(test_min, len(flows[k]))\n",
        "\n",
        "# \ttrain_keys = set(keys) - set(test_keys)\n",
        "# \ttrain_min = 0xFFFFFFFF\n",
        "# \ttrain_flows = {}\n",
        "# \tfor k in train_keys:\n",
        "# \t\ttrain_flows[k] = flows[k]\n",
        "# \t\ttrain_min = min(train_min, len(flows[k]))\n",
        "\n",
        "# \tprint('============================')\n",
        "# \tprint('Generate flows for %s'%name)\n",
        "# \tprint('Total flows: ', len(flows))\n",
        "# \tprint('Train flows: ', len(train_flows), ' Min pkts: ', train_min)\n",
        "# \tprint('Test flows: ', len(test_flows), ' Min pkts: ', test_min)\n",
        "\n",
        "# \treturn train_flows, test_flows\n",
        "\n",
        "\n",
        "def closure(flows):\n",
        "\tflow_dict = {}\n",
        "\tfor name in protocols:\n",
        "\t\tindex = protocols.index(name)\n",
        "\t\tflow_dict[name] = flows[index]\n",
        "\t\tprint('============================')\n",
        "\t\tprint('Generate flows for %s'%name)\n",
        "\t\tprint('Total flows: ', len(flows[index]))\n",
        "\t\tcnt = 0\n",
        "\t\tfor k, v in flows[index].items():\n",
        "\t\t\tcnt += len(v)\n",
        "\t\tprint('Total pkts: ', cnt)\n",
        "\n",
        "\twith open('/content/drive/MyDrive/Colab Notebooks/SAM_learn/pro_flows.pkl', 'wb') as f:\n",
        "\t\tpickle.dump(flow_dict, f)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "pcap = dpkt.pcap.Reader(open('/content/drive/MyDrive/Colab Notebooks/SAM_learn/aim_chat_3a.pcap', 'rb'))\n",
        "flows = gen_flows(pcap)\n",
        "closure(flows)"
      ],
      "metadata": {
        "id": "jEkQ1provSgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/SAM_learn/pro_flows.pkl', 'rb') as g:   \n",
        "    data = pickle.load(g)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "v2j6DSvoxLkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Tool"
      ],
      "metadata": {
        "id": "TVP6LKb3yU_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import dpkt\n",
        "import random\n",
        "import numpy as np\n",
        "# from preprocess import protocols\n",
        "from tqdm import tqdm, trange"
      ],
      "metadata": {
        "id": "LYp8QjW9xtpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ip_features = {'hl':1,'tos':1,'len':2,'df':1,'mf':1,'ttl':1,'p':1}\n",
        "tcp_features = {'off':1,'flags':1,'win':2}\n",
        "udp_features = {'ulen':2}\n",
        "max_byte_len = 50\n",
        "\n",
        "def mask(p):\n",
        "\tp.src = b'\\x00\\x00\\x00\\x00'\n",
        "\tp.dst = b'\\x00\\x00\\x00\\x00'\n",
        "\tp.sum = 0\n",
        "\tp.id = 0\n",
        "\tp.offset = 0\n",
        "\n",
        "\tif isinstance(p.data, dpkt.tcp.TCP):\n",
        "\t\tp.data.sport = 0\n",
        "\t\tp.data.dport = 0\n",
        "\t\tp.data.seq = 0\n",
        "\t\tp.data.ack = 0\n",
        "\t\tp.data.sum = 0\n",
        "\n",
        "\telif isinstance(p.data, dpkt.udp.UDP):\n",
        "\t\tp.data.sport = 0\n",
        "\t\tp.data.dport = 0\n",
        "\t\tp.data.sum = 0\n",
        "\n",
        "\treturn p\n",
        "\n",
        "def pkt2feature(data, k):\n",
        "\tflow_dict = {'train':{}, 'test':{}}\n",
        "\n",
        "\t# train->protocol->flowid->[pkts]\n",
        "\tfor p in protocols:\n",
        "\t\tflow_dict['train'][p] = []\n",
        "\t\tflow_dict['test'][p] = []\n",
        "\t\tall_pkts = []\n",
        "\t\tp_keys = list(data[p].keys())\n",
        "\n",
        "\t\tfor flow in p_keys:\n",
        "\t\t\tpkts = data[p][flow]\n",
        "\t\t\tall_pkts.extend(pkts)\n",
        "\t\trandom.Random(1024).shuffle(all_pkts)\n",
        "\n",
        "\t\tfor idx in range(len(all_pkts)):\n",
        "\t\t\tpkt = mask(all_pkts[idx])\n",
        "\t\t\traw_byte = pkt.pack()\n",
        "\n",
        "\t\t\tbyte = []\n",
        "\t\t\tpos = []\n",
        "\t\t\tfor x in range(min(len(raw_byte),max_byte_len)):\n",
        "\t\t\t\tbyte.append(int(raw_byte[x]))\n",
        "\t\t\t\tpos.append(x)\n",
        "\n",
        "\t\t\tbyte.extend([0]*(max_byte_len-len(byte)))\n",
        "\t\t\tpos.extend([0]*(max_byte_len-len(pos)))\n",
        "\t\t\t# if len(byte) != max_byte_len or len(pos) != max_byte_len:\n",
        "\t\t\t# \tprint(len(byte), len(pos))\n",
        "\t\t\t# \tinput()\n",
        "\t\t\tif idx in range(k*int(len(all_pkts)*0.1), (k+1)*int(len(all_pkts)*0.1)):\n",
        "\t\t\t\tflow_dict['test'][p].append((byte, pos))\n",
        "\t\t\telse:\n",
        "\t\t\t\tflow_dict['train'][p].append((byte, pos))\n",
        "\treturn flow_dict\n",
        "\n",
        "def load_epoch_data(flow_dict, train='train'):\n",
        "\tflow_dict = flow_dict[train]\n",
        "\tx, y, label = [], [], []\n",
        "\n",
        "\tfor p in protocols:\n",
        "\t\tpkts = flow_dict[p]\n",
        "\t\tfor byte, pos in pkts:\n",
        "\t\t\tx.append(byte)\n",
        "\t\t\ty.append(pos)\n",
        "\t\t\tlabel.append(protocols.index(p))\n",
        "\n",
        "\treturn np.array(x), np.array(y), np.array(label)[:, np.newaxis]\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "\t# f = open('flows.pkl','rb')\n",
        "\t# data = pickle.load(f)\n",
        "\t# f.close()\n",
        "\n",
        "\t# print(data.keys())\n",
        "\n",
        "\t# dns = data['dns']\n",
        "\t# # print(list(dns.keys())[:10])\n",
        "\n",
        "\t# # wide dataset contains payload\n",
        "\t# print('================\\n',\n",
        "\t# \tlen(dns['203.206.160.197.202.89.157.51.17.53.51648'][0]))\n",
        "\n",
        "\t# print('================')\n",
        "\t# flow_dict = pkt2feature(data)\n",
        "\t# x, y, label = train_epoch_data(flow_dict)\n",
        "\t# print(x.shape)\n",
        "\t# print(y.shape)\n",
        "\t# print(label[0])\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/SAM_learn/pro_flows.pkl','rb') as f:\n",
        "\tdata = pickle.load(f)\n",
        "\n",
        "for i in trange(10, mininterval=2, \\\n",
        "\tdesc='  - (Building fold dataset)   ', leave=False):\n",
        "\tflow_dict = pkt2feature(data, i)\n",
        "\twith open('/content/drive/MyDrive/Colab Notebooks/SAM_learn/pro_flows_%d_noip_fold.pkl'%i, 'wb') as f:\n",
        "\t\tpickle.dump(flow_dict, f)"
      ],
      "metadata": {
        "id": "DIwOEv6nyDOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/SAM_learn/pro_flows_0_noip_fold.pkl', 'rb') as g:   \n",
        "    data1 = pickle.load(g)\n",
        "\n",
        "data1"
      ],
      "metadata": {
        "id": "aCpLIVJtzVNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. SAM"
      ],
      "metadata": {
        "id": "w8ryWfka8Bss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "B4W_FaOB9luO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(2020)\n",
        "torch.cuda.manual_seed_all(2020)\n",
        "np.random.seed(2020)\n",
        "random.seed(2020)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\t\"\"\"docstring for SelfAttention\"\"\"\n",
        "\tdef __init__(self, d_dim=256, dropout=0.1):\n",
        "\t\tsuper(SelfAttention, self).__init__()\n",
        "\t\t# for query, key, value, output\n",
        "\t\tself.dim = d_dim\n",
        "\t\tself.linears = nn.ModuleList([nn.Linear(d_dim, d_dim) for _ in range(4)])\n",
        "\t\tself.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\tdef attention(self, query, key, value):\n",
        "\t\tscores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.dim)\n",
        "\t\tscores = F.softmax(scores, dim=-1)\n",
        "\t\treturn scores\n",
        "\n",
        "\tdef forward(self, query, key, value):\n",
        "\t\t# 1) query, key, value\n",
        "\t\tquery, key, value = \\\n",
        "\t\t[l(x) for l, x in zip(self.linears, (query, key, value))]\n",
        "\n",
        "\t\t# 2) Apply attention\n",
        "\t\tscores = self.attention(query, key, value)\n",
        "\t\tx = torch.matmul(scores, value)\n",
        "\n",
        "\t\t# 3) apply the final linear\n",
        "\t\tx = self.linears[-1](x.contiguous())\n",
        "\t\t# sum keepdim=False\n",
        "\t\treturn self.dropout(x), torch.mean(scores, dim=-2)\n",
        "\n",
        "class OneDimCNN(nn.Module):\n",
        "\t\"\"\"docstring for OneDimCNN\"\"\"\n",
        "\t# https://blog.csdn.net/sunny_xsc1994/article/details/82969867\n",
        "\tdef __init__(self, max_byte_len, d_dim=256, \\\n",
        "\t\tkernel_size = [3, 4], filters=256, dropout=0.1):\n",
        "\t\tsuper(OneDimCNN, self).__init__()\n",
        "\t\tself.kernel_size = kernel_size\n",
        "\t\tself.convs = nn.ModuleList([\n",
        "\t\t\t\t\t\tnn.Sequential(nn.Conv1d(in_channels=d_dim, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\tout_channels=filters, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\tkernel_size=h),\n",
        "\t\t\t\t\t\t#nn.BatchNorm1d(num_features=config.feature_size), \n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t# MaxPool1d: \n",
        "\t\t\t\t\t\t# stride â€“ the stride of the window. Default value is kernel_size\n",
        "\t\t\t\t\t\tnn.MaxPool1d(kernel_size=max_byte_len-h+1))\n",
        "\t\t\t\t\t\tfor h in self.kernel_size\n",
        "\t\t\t\t\t\t]\n",
        "\t\t\t\t\t\t)\n",
        "\t\tself.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tout = [conv(x.transpose(-2,-1)) for conv in self.convs]\n",
        "\t\tout = torch.cat(out, dim=1)\n",
        "\t\tout = out.view(-1, out.size(1))\n",
        "\t\treturn self.dropout(out)\n",
        "\n",
        "\n",
        "class SAM(nn.Module):\n",
        "\t\"\"\"docstring for SAM\"\"\"\n",
        "\t# total header bytes 24\n",
        "\tdef __init__(self, num_class, max_byte_len, kernel_size = [3, 4], \\\n",
        "\t\td_dim=256, dropout=0.1, filters=256):\n",
        "\t\tsuper(SAM, self).__init__()\n",
        "\t\tself.posembedding = nn.Embedding(num_embeddings=max_byte_len, \n",
        "\t\t\t\t\t\t\t\t            embedding_dim=d_dim)\n",
        "\t\tself.byteembedding = nn.Embedding(num_embeddings=300, \n",
        "\t\t\t\t\t\t\t\t            embedding_dim=d_dim)\n",
        "\t\tself.attention = SelfAttention(d_dim, dropout)\n",
        "\t\tself.cnn = OneDimCNN(max_byte_len, d_dim, kernel_size, filters, dropout)\n",
        "\t\tself.fc = nn.Linear(in_features=256*len(kernel_size),\n",
        "                            out_features=num_class)\n",
        "\n",
        "\tdef forward(self, x, y):\n",
        "\t\tout = self.byteembedding(x) + self.posembedding(y)\n",
        "\t\tout, score = self.attention(out, out, out)\n",
        "\t\tout = self.cnn(out)\n",
        "\t\tout = self.fc(out)\n",
        "\t\tif not self.training:\n",
        "\t\t\treturn F.softmax(out, dim=-1).max(1)[1], score\n",
        "\t\treturn out\n",
        "\t\t\n",
        "#if __name__ == '__main__':\n",
        "#    x = np.random.randint(0, 255, (10, 20))\n",
        "#    y = np.random.randint(0, 20, (10, 20))\n",
        "#    sam = SAM(num_class=5, max_byte_len=20)\n",
        "#    out = sam(torch.from_numpy(x).long(), torch.from_numpy(y).long())\n",
        "#    print(out[0])\n",
        "\n",
        "#    sam.eval()\n",
        "#    out, score = sam(torch.from_numpy(x).long(), torch.from_numpy(y).long())\n",
        "#    print(out[0], score[0])\n"
      ],
      "metadata": {
        "id": "nE4vlp0P8BCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Train"
      ],
      "metadata": {
        "id": "OEYjp5TH0deN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LCcbVpNa0dc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/SAM_learn')"
      ],
      "metadata": {
        "id": "FVmA9ZDI1xt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import cuda\n",
        "import torch.utils.data\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import time\n",
        "from tqdm import tqdm, trange\n",
        "# from SAM import SAM\n",
        "# from SAM import *\n",
        "from tool import protocols, load_epoch_data, max_byte_len\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "import pickle\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "29_wMHsG0iyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "rYFJdAv-5BmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\t\"\"\"docstring for Dataset\"\"\"\n",
        "\tdef __init__(self, x, y, label):\n",
        "\t\tsuper(Dataset, self).__init__()\n",
        "\t\tself.x = x\n",
        "\t\tself.y = y\n",
        "\t\tself.label = label\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.x)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\treturn self.x[idx], self.y[idx], self.label[idx]\n",
        "\n",
        "def paired_collate_fn(insts):\n",
        "\tx, y, label = list(zip(*insts))\n",
        "\treturn torch.LongTensor(x), torch.LongTensor(y), torch.LongTensor(label)\n",
        "\n",
        "def cal_loss(pred, gold, cls_ratio=None):\n",
        "\tgold = gold.contiguous().view(-1)\n",
        "\t# By default, the losses are averaged over each loss element in the batch. \n",
        "\tloss = F.cross_entropy(pred, gold)\n",
        "\n",
        "\t# torch.max(a,0) \n",
        "\tpred = F.softmax(pred, dim = -1).max(1)[1]\n",
        "\t# \n",
        "\tn_correct = pred.eq(gold)\n",
        "\tacc = n_correct.sum().item() / n_correct.shape[0]\n",
        "\n",
        "\treturn loss, acc*100\n",
        "\n",
        "def test_epoch(model, test_data):\n",
        "\t''' Epoch operation in training phase'''\n",
        "\tmodel.eval()\n",
        "\n",
        "\ttotal_acc = []\n",
        "\ttotal_pred = []\n",
        "\ttotal_score = []\n",
        "\ttotal_time = []\n",
        "\n",
        "\tfor batch in tqdm(\n",
        "\t\ttest_data, mininterval=2,\n",
        "\t\tdesc='  - (Testing)   ', leave=False):\n",
        "\n",
        "\t\t# prepare data\n",
        "\t\tsrc_seq, src_seq2, gold = batch\n",
        "\t\tsrc_seq, src_seq2, gold = src_seq.cuda(), src_seq2.cuda(), gold.cuda()\n",
        "\t\tgold = gold.contiguous().view(-1)\n",
        "\n",
        "\t\t# forward\n",
        "\t\ttorch.cuda.synchronize()\n",
        "\t\tstart = time.time()\n",
        "\t\tpred, score = model(src_seq, src_seq2)\n",
        "\t\ttorch.cuda.synchronize()\n",
        "\t\tend = time.time()\n",
        "\t\t# \n",
        "\t\tn_correct = pred.eq(gold)\n",
        "\t\tacc = n_correct.sum().item()*100 / n_correct.shape[0]\n",
        "\t\ttotal_acc.append(acc)\n",
        "\t\ttotal_pred.extend(pred.long().tolist())\n",
        "\t\ttotal_score.append(torch.mean(score, dim=0).tolist())\n",
        "\t\ttotal_time.append(end - start)\n",
        "\n",
        "\treturn sum(total_acc)/len(total_acc), np.array(total_score).mean(axis=0), \\\n",
        "\ttotal_pred, sum(total_time)/len(total_time)\n",
        "\n",
        "def train_epoch(model, training_data, optimizer):\n",
        "\t''' Epoch operation in training phase'''\n",
        "\tmodel.train()\n",
        "\n",
        "\ttotal_loss = []\n",
        "\ttotal_acc = []\n",
        "\n",
        "\tfor batch in tqdm(\n",
        "\t\ttraining_data, mininterval=2,\n",
        "\t\tdesc='  - (Training)   ', leave=False):\n",
        "\n",
        "\t\t# prepare data\n",
        "\t\tsrc_seq, src_seq2, gold = batch\n",
        "\t\tsrc_seq, src_seq2, gold = src_seq.cuda(), src_seq2.cuda(), gold.cuda()\n",
        "\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\t# forward\n",
        "\t\tpred = model(src_seq, src_seq2)\n",
        "\t\tloss_per_batch, acc_per_batch = cal_loss(pred, gold)\n",
        "\t\t# update parameters\n",
        "\t\tloss_per_batch.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\t# \n",
        "\t\ttotal_loss.append(loss_per_batch.item())\n",
        "\t\ttotal_acc.append(acc_per_batch)\n",
        "\n",
        "\treturn sum(total_loss)/len(total_loss), sum(total_acc)/len(total_acc)\n",
        "\n",
        "def main(i, flow_dict):\n",
        "\tf = open('/content/drive/MyDrive/Colab Notebooks/SAM_learn/results_%d.txt'%i, 'w')\n",
        "\tf.write('Train Loss Time Test\\n')\n",
        "\tf.flush()\n",
        "\n",
        "\n",
        "\tmodel = SAM(num_class=len(protocols), max_byte_len=max_byte_len).cuda()\n",
        "\toptimizer = optim.Adam(filter(lambda x: x.requires_grad, model.parameters()))\n",
        "\tloss_list = []\n",
        "\t# default epoch is 3\n",
        "\tfor epoch_i in trange(5, mininterval=2, \\\n",
        "\t\tdesc='  - (Training Epochs)   ', leave=False):\n",
        "\n",
        "\t\ttrain_x, train_y, train_label = load_epoch_data(flow_dict, 'train')\n",
        "\t\ttraining_data = torch.utils.data.DataLoader(\n",
        "\t\t\t\tDataset(x=train_x, y=train_y, label=train_label),\n",
        "\t\t\t\tnum_workers=0,\n",
        "\t\t\t\tcollate_fn=paired_collate_fn,\n",
        "\t\t\t\tbatch_size=128,\n",
        "\t\t\t\tshuffle=True\n",
        "\t\t\t)\n",
        "\t\ttrain_loss, train_acc = train_epoch(model, training_data, optimizer)\n",
        "\n",
        "\t\ttest_x, test_y, test_label = load_epoch_data(flow_dict, 'test')\n",
        "\t\ttest_data = torch.utils.data.DataLoader(\n",
        "\t\t\t\tDataset(x=test_x, y=test_y, label=test_label),\n",
        "\t\t\t\tnum_workers=0,\n",
        "\t\t\t\tcollate_fn=paired_collate_fn,\n",
        "\t\t\t\tbatch_size=128,\n",
        "\t\t\t\tshuffle=False\n",
        "\t\t\t)\n",
        "\t\ttest_acc, score, pred, test_time = test_epoch(model, test_data)\n",
        "\t\twith open('/content/drive/MyDrive/Colab Notebooks/SAM_learn/atten_%d.txt'%i, 'w') as f2:\n",
        "\t\t\tf2.write(' '.join(map('{:.4f}'.format, score)))\n",
        "\n",
        "\t\t# write F1, PRECISION, RECALL\n",
        "\t\twith open('/content/drive/MyDrive/Colab Notebooks/SAM_learn/metric_%d.txt'%i, 'w') as f3:\n",
        "\t\t\tf3.write('F1 PRE REC\\n')\n",
        "\t\t\tp, r, fscore, _ = precision_recall_fscore_support(test_label, pred)\n",
        "\t\t\tfor a, b, c in zip(fscore, p, r):\n",
        "\t\t\t\t# for every cls\n",
        "\t\t\t\tf3.write('%.2f %.2f %.2f\\n'%(a, b, c))\n",
        "\t\t\t\tf3.flush()\n",
        "\t\t\tif len(fscore) != len(protocols):\n",
        "\t\t\t\ta = set(pred)\n",
        "\t\t\t\tb = set(test_label[:,0])\n",
        "\t\t\t\tf3.write('%s\\n%s'%(str(a), str(b)))\n",
        "\n",
        "\t\t# write Confusion Matrix\n",
        "\t\twith open('/content/drive/MyDrive/Colab Notebooks/SAM_learn/cm_%d.pkl'%i, 'wb') as f4:\n",
        "\t\t\tpickle.dump(confusion_matrix(test_label, pred, normalize='true'), f4)\n",
        "\n",
        "\n",
        "\t\t# write ACC\n",
        "\t\tf.write('%.2f %.4f %.6f %.2f\\n'%(train_acc, train_loss, test_time, test_acc))\n",
        "\t\tf.flush()\n",
        "\n",
        "\t\t# # early stop\n",
        "\t\t# if len(loss_list) == 5:\n",
        "\t\t# \tif abs(sum(loss_list)/len(loss_list) - train_loss) < 0.005:\n",
        "\t\t# \t\tbreak\n",
        "\t\t# \tloss_list[epoch_i%len(loss_list)] = train_loss\n",
        "\t\t# else:\n",
        "\t\t# \tloss_list.append(train_loss)\n",
        "\n",
        "\tf.close()\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "for i in range(10):\n",
        "\twith open('/content/drive/MyDrive/Colab Notebooks/SAM_learn/pro_flows_%d_noip_fold.pkl'%i, 'rb') as f:\n",
        "\t\tflow_dict = pickle.load(f)\n",
        "\tprint('====', i, ' fold validation ====')\n",
        "\tmain(i, flow_dict)"
      ],
      "metadata": {
        "id": "JmOpn_Wx1-H9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/SAM_learn/cm_0.pkl', 'rb') as g:   \n",
        "    data2 = pickle.load(g)\n",
        "\n",
        "data2"
      ],
      "metadata": {
        "id": "byiKJOl165_s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}